{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1fmdVWyrKyA3n2emT5mLCGAu6uPzlE-yj",
      "authorship_tag": "ABX9TyNuldu1ItRZaoStfaRkXJlG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MWH997/CovidMask-NeuralNetwork-Analysis/blob/master/Comparative_Analysis_of_Neural_Networks_for_COVID_Mask_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "\n",
        "# Comparative Analysis of Neural Networks for COVID Mask Classification\n",
        "\n",
        "<center>"
      ],
      "metadata": {
        "id": "sxThsCv-W2hk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "\n",
        "## Md Wahid Hassan\n",
        "\n",
        "<center>"
      ],
      "metadata": {
        "id": "slQBx4PKXAMG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1"
      ],
      "metadata": {
        "id": "b-UY9bVXWNN4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report"
      ],
      "metadata": {
        "id": "TKqOh85svKhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class COVIDMaskDataset(Dataset):\n",
        "    def __init__(self, x=None, y=None):\n",
        "        self.x = torch.load(\"/content/drive/MyDrive/COVID_Mask/COVID_Mask_Images.pt\") if not x else x\n",
        "        self.y = torch.load(\"/content/drive/MyDrive/COVID_Mask/COVID_Mask_Labels.pt\") if not y else y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.x[idx], self.y[idx]"
      ],
      "metadata": {
        "id": "mgHSgc_0fP96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class bFCNN(nn.Module):\n",
        "    # Define the basic Simple Neural Network here\n",
        "    def __init__(self, input_size=3*128*128, hidden_size=128, num_classes=3):\n",
        "        super(bFCNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1) # flatten input tensor except batch dimension\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "Ns7G8mLKfP6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The bFCNN is a simple neural network that takes an input image, processes it through a single hidden layer with 128 neurons and a ReLU activation function, and outputs a tensor containing the scores for three different classes. It can be used for basic image classification tasks involving 128x128 RGB images."
      ],
      "metadata": {
        "id": "At0YxiIgQ1Vq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class iFCNN(nn.Module):\n",
        "    # Define the improved Simple Neural Network with four hidden layers\n",
        "    def __init__(self, input_size=3*128*128, hidden_size1=1024, hidden_size2=512, hidden_size3=256, hidden_size4=128, num_classes=3):\n",
        "        super(iFCNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(hidden_size2, hidden_size3)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc4 = nn.Linear(hidden_size3, hidden_size4)\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc5 = nn.Linear(hidden_size4, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, channels, height, width = x.size()\n",
        "        x = x.view(batch_size, -1)  # Flatten the input tensor except for the batch dimension\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.relu3(x)\n",
        "        x = self.fc4(x)\n",
        "        x = self.relu4(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc5(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "ZlAXBu4zfP2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The iFCNN is an improved version of the simple neural network, designed for image classification tasks involving 128x128 RGB images. It consists of four hidden layers with 1024, 512, 256, and 128 neurons respectively, each followed by a ReLU activation function. After the final hidden layer, a dropout layer with a rate of 0.5 is added to help prevent overfitting. Finally, a fully connected output layer produces the scores for the three different classes. This neural network architecture aims to provide better performance by increasing the model's complexity and utilizing dropout for regularization."
      ],
      "metadata": {
        "id": "7RIioe5BRGwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class bCNN(nn.Module):\n",
        "    # Define the basic Convolutional Neural Network here\n",
        "    def __init__(self, num_classes=3):\n",
        "        super(bCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(6 * 62 * 62, 120)\n",
        "        self.fc2 = nn.Linear(120, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, channels, height, width = x.size()\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = torch.flatten(x, 1)  # flatten all dimensions except batch\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "jDUVHXKAfPph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The bCNN, or basic Convolutional Neural Network, is a straightforward deep learning model designed for classifying 128x128 RGB images into one of three categories. The model extracts meaningful patterns from the input images using a convolutional layer, which applies filters to capture spatial features. It then uses a max-pooling layer to reduce the dimensions of the feature maps, summarizing the most important information while minimizing computational complexity. Finally, the model employs fully connected layers to integrate the extracted features and produce a final classification."
      ],
      "metadata": {
        "id": "YZPFVVgTRika"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class iCNN(nn.Module):\n",
        "    # Define the improved Convolutional Neural Network here\n",
        "    def __init__(self, num_classes=3):\n",
        "        super(iCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.conv3 = nn.Conv2d(16, 32, 5)\n",
        "\n",
        "        # Calculate the output size after each layer\n",
        "        conv1_out = (128 - 5 + 1) // 2  # Output size after conv1 and pool\n",
        "        conv2_out = (conv1_out - 5 + 1) // 2  # Output size after conv2 and pool\n",
        "        conv3_out = (conv2_out - 5 + 1) // 2  # Output size after conv3 and pool\n",
        "\n",
        "        self.fc1 = nn.Linear(32 * conv3_out * conv3_out, 512)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, channels, height, width = x.size()\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = torch.flatten(x, 1)  # Flatten all dimensions except batch\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "XhOk4OxHfPf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The iCNN, or improved Convolutional Neural Network, is an advanced deep learning model designed to classify 128x128 RGB images into one of three categories. This model enhances the basic Convolutional Neural Network by incorporating additional convolutional layers and a dropout layer to prevent overfitting.\n",
        "\n",
        "The model begins with a series of three convolutional layers, each followed by a max-pooling layer. These layers extract spatial features from the input images and progressively reduce their dimensions while retaining crucial information. The additional convolutional layers enable the model to capture more complex patterns, leading to improved performance.\n",
        "\n",
        "After the convolutional layers, the model employs a fully connected layer to consolidate the extracted features. A dropout layer is then used to randomly set a fraction of input units to zero during training, which helps prevent overfitting by encouraging the model to learn a more robust representation of the data. Finally, another fully connected layer produces the final classification output.\n"
      ],
      "metadata": {
        "id": "OXKXjOjVR6Pq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, (inputs, labels) in enumerate(dataloader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    return running_loss / len(dataloader)"
      ],
      "metadata": {
        "id": "nCSiXKGrfPcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return correct / total"
      ],
      "metadata": {
        "id": "ZescDbKwfPZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_predictions(model, dataloader, device):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, labels in dataloader:\n",
        "            data, labels = data.to(device), labels.to(device)\n",
        "            outputs = model(data)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    return all_labels, all_preds"
      ],
      "metadata": {
        "id": "gdemV72gHW-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "dataset = COVIDMaskDataset()\n",
        "\n",
        "train_indices, test_indices = train_test_split(range(len(dataset)), test_size=0.2, random_state=42)\n",
        "train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
        "test_dataset = torch.utils.data.Subset(dataset, test_indices)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "vt2mig4Fme1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 50"
      ],
      "metadata": {
        "id": "XZH4mOXaHgT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = ''\n",
        "models = [bFCNN(), iFCNN(), bCNN(), iCNN()]\n",
        "model_names = [\"bFCNN\", \"iFCNN\", \"bCNN\", \"iCNN\"]\n",
        "\n",
        "\n",
        "for model, name in zip(models, model_names):\n",
        "    print('\\n\\n')\n",
        "    print(f\"Training {name}...\")\n",
        "    model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss = train(model, train_dataloader, criterion, optimizer, device)\n",
        "        print(f\"[{epoch+1}/{num_epochs}] Train Loss: {train_loss:.4f}\")\n",
        "\n",
        "    test_accuracy = evaluate(model, test_dataloader, device)\n",
        "    print(f\"\\n{name} Test Accuracy: {test_accuracy:.4f}\\n\")\n",
        "\n",
        "    # Get predictions and labels\n",
        "    labels, preds = get_predictions(model, test_dataloader, device)\n",
        "\n",
        "    # Print the classification report\n",
        "    print(f\"{name} Classification Report:\")\n",
        "    print(classification_report(labels, preds))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMc14OEjwsxD",
        "outputId": "bf9c84cc-f419-40cf-fa9b-6940c7645d1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "Training bFCNN...\n",
            "[1/50] Train Loss: 0.9745\n",
            "[2/50] Train Loss: 0.4634\n",
            "[3/50] Train Loss: 0.3577\n",
            "[4/50] Train Loss: 0.3618\n",
            "[5/50] Train Loss: 0.3121\n",
            "[6/50] Train Loss: 0.2878\n",
            "[7/50] Train Loss: 0.2763\n",
            "[8/50] Train Loss: 0.2152\n",
            "[9/50] Train Loss: 0.2230\n",
            "[10/50] Train Loss: 0.2096\n",
            "[11/50] Train Loss: 0.1981\n",
            "[12/50] Train Loss: 0.1801\n",
            "[13/50] Train Loss: 0.1613\n",
            "[14/50] Train Loss: 0.1743\n",
            "[15/50] Train Loss: 0.1674\n",
            "[16/50] Train Loss: 0.1469\n",
            "[17/50] Train Loss: 0.1546\n",
            "[18/50] Train Loss: 0.1458\n",
            "[19/50] Train Loss: 0.1516\n",
            "[20/50] Train Loss: 0.1427\n",
            "[21/50] Train Loss: 0.1382\n",
            "[22/50] Train Loss: 0.0989\n",
            "[23/50] Train Loss: 0.1577\n",
            "[24/50] Train Loss: 0.1055\n",
            "[25/50] Train Loss: 0.0992\n",
            "[26/50] Train Loss: 0.1078\n",
            "[27/50] Train Loss: 0.1012\n",
            "[28/50] Train Loss: 0.1042\n",
            "[29/50] Train Loss: 0.0906\n",
            "[30/50] Train Loss: 0.0701\n",
            "[31/50] Train Loss: 0.0902\n",
            "[32/50] Train Loss: 0.1164\n",
            "[33/50] Train Loss: 0.1102\n",
            "[34/50] Train Loss: 0.1071\n",
            "[35/50] Train Loss: 0.0900\n",
            "[36/50] Train Loss: 0.0676\n",
            "[37/50] Train Loss: 0.0688\n",
            "[38/50] Train Loss: 0.0785\n",
            "[39/50] Train Loss: 0.1057\n",
            "[40/50] Train Loss: 0.0557\n",
            "[41/50] Train Loss: 0.0853\n",
            "[42/50] Train Loss: 0.0593\n",
            "[43/50] Train Loss: 0.0631\n",
            "[44/50] Train Loss: 0.0638\n",
            "[45/50] Train Loss: 0.0875\n",
            "[46/50] Train Loss: 0.0601\n",
            "[47/50] Train Loss: 0.1215\n",
            "[48/50] Train Loss: 0.0816\n",
            "[49/50] Train Loss: 0.0606\n",
            "[50/50] Train Loss: 0.0595\n",
            "\n",
            "bFCNN Test Accuracy: 0.9572\n",
            "\n",
            "bFCNN Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      1.00      0.98       620\n",
            "           1       0.99      0.89      0.94       613\n",
            "           2       0.92      0.98      0.95       564\n",
            "\n",
            "    accuracy                           0.96      1797\n",
            "   macro avg       0.96      0.96      0.96      1797\n",
            "weighted avg       0.96      0.96      0.96      1797\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training iFCNN...\n",
            "[1/50] Train Loss: 0.7681\n",
            "[2/50] Train Loss: 0.4777\n",
            "[3/50] Train Loss: 0.3800\n",
            "[4/50] Train Loss: 0.3334\n",
            "[5/50] Train Loss: 0.2771\n",
            "[6/50] Train Loss: 0.2534\n",
            "[7/50] Train Loss: 0.2292\n",
            "[8/50] Train Loss: 0.2184\n",
            "[9/50] Train Loss: 0.2002\n",
            "[10/50] Train Loss: 0.1663\n",
            "[11/50] Train Loss: 0.1520\n",
            "[12/50] Train Loss: 0.1445\n",
            "[13/50] Train Loss: 0.1306\n",
            "[14/50] Train Loss: 0.1423\n",
            "[15/50] Train Loss: 0.1369\n",
            "[16/50] Train Loss: 0.1193\n",
            "[17/50] Train Loss: 0.1254\n",
            "[18/50] Train Loss: 0.1210\n",
            "[19/50] Train Loss: 0.1195\n",
            "[20/50] Train Loss: 0.0903\n",
            "[21/50] Train Loss: 0.0889\n",
            "[22/50] Train Loss: 0.1283\n",
            "[23/50] Train Loss: 0.1109\n",
            "[24/50] Train Loss: 0.0999\n",
            "[25/50] Train Loss: 0.1024\n",
            "[26/50] Train Loss: 0.1048\n",
            "[27/50] Train Loss: 0.0885\n",
            "[28/50] Train Loss: 0.0817\n",
            "[29/50] Train Loss: 0.0809\n",
            "[30/50] Train Loss: 0.1017\n",
            "[31/50] Train Loss: 0.0733\n",
            "[32/50] Train Loss: 0.0905\n",
            "[33/50] Train Loss: 0.0634\n",
            "[34/50] Train Loss: 0.0669\n",
            "[35/50] Train Loss: 0.0863\n",
            "[36/50] Train Loss: 0.0649\n",
            "[37/50] Train Loss: 0.0838\n",
            "[38/50] Train Loss: 0.0878\n",
            "[39/50] Train Loss: 0.0816\n",
            "[40/50] Train Loss: 0.0932\n",
            "[41/50] Train Loss: 0.0873\n",
            "[42/50] Train Loss: 0.0698\n",
            "[43/50] Train Loss: 0.0853\n",
            "[44/50] Train Loss: 0.0704\n",
            "[45/50] Train Loss: 0.0543\n",
            "[46/50] Train Loss: 0.0505\n",
            "[47/50] Train Loss: 0.0641\n",
            "[48/50] Train Loss: 0.0602\n",
            "[49/50] Train Loss: 0.0724\n",
            "[50/50] Train Loss: 0.0550\n",
            "\n",
            "iFCNN Test Accuracy: 0.9577\n",
            "\n",
            "iFCNN Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.99      0.97       620\n",
            "           1       0.96      0.94      0.95       613\n",
            "           2       0.98      0.94      0.96       564\n",
            "\n",
            "    accuracy                           0.96      1797\n",
            "   macro avg       0.96      0.96      0.96      1797\n",
            "weighted avg       0.96      0.96      0.96      1797\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training bCNN...\n",
            "[1/50] Train Loss: 0.3564\n",
            "[2/50] Train Loss: 0.1854\n",
            "[3/50] Train Loss: 0.1313\n",
            "[4/50] Train Loss: 0.0878\n",
            "[5/50] Train Loss: 0.0656\n",
            "[6/50] Train Loss: 0.0351\n",
            "[7/50] Train Loss: 0.0283\n",
            "[8/50] Train Loss: 0.0360\n",
            "[9/50] Train Loss: 0.0227\n",
            "[10/50] Train Loss: 0.0182\n",
            "[11/50] Train Loss: 0.0336\n",
            "[12/50] Train Loss: 0.0342\n",
            "[13/50] Train Loss: 0.0098\n",
            "[14/50] Train Loss: 0.0085\n",
            "[15/50] Train Loss: 0.0026\n",
            "[16/50] Train Loss: 0.0017\n",
            "[17/50] Train Loss: 0.0588\n",
            "[18/50] Train Loss: 0.0498\n",
            "[19/50] Train Loss: 0.0081\n",
            "[20/50] Train Loss: 0.0019\n",
            "[21/50] Train Loss: 0.0008\n",
            "[22/50] Train Loss: 0.0003\n",
            "[23/50] Train Loss: 0.0001\n",
            "[24/50] Train Loss: 0.0001\n",
            "[25/50] Train Loss: 0.0001\n",
            "[26/50] Train Loss: 0.0001\n",
            "[27/50] Train Loss: 0.0001\n",
            "[28/50] Train Loss: 0.0001\n",
            "[29/50] Train Loss: 0.0000\n",
            "[30/50] Train Loss: 0.0000\n",
            "[31/50] Train Loss: 0.0000\n",
            "[32/50] Train Loss: 0.0000\n",
            "[33/50] Train Loss: 0.0000\n",
            "[34/50] Train Loss: 0.0000\n",
            "[35/50] Train Loss: 0.0000\n",
            "[36/50] Train Loss: 0.0000\n",
            "[37/50] Train Loss: 0.0000\n",
            "[38/50] Train Loss: 0.0000\n",
            "[39/50] Train Loss: 0.0000\n",
            "[40/50] Train Loss: 0.0000\n",
            "[41/50] Train Loss: 0.0000\n",
            "[42/50] Train Loss: 0.0000\n",
            "[43/50] Train Loss: 0.0000\n",
            "[44/50] Train Loss: 0.0000\n",
            "[45/50] Train Loss: 0.0000\n",
            "[46/50] Train Loss: 0.0000\n",
            "[47/50] Train Loss: 0.0000\n",
            "[48/50] Train Loss: 0.0000\n",
            "[49/50] Train Loss: 0.0000\n",
            "[50/50] Train Loss: 0.0000\n",
            "\n",
            "bCNN Test Accuracy: 0.9805\n",
            "\n",
            "bCNN Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      1.00      0.99       620\n",
            "           1       0.98      0.96      0.97       613\n",
            "           2       0.98      0.98      0.98       564\n",
            "\n",
            "    accuracy                           0.98      1797\n",
            "   macro avg       0.98      0.98      0.98      1797\n",
            "weighted avg       0.98      0.98      0.98      1797\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training iCNN...\n",
            "[1/50] Train Loss: 0.4418\n",
            "[2/50] Train Loss: 0.2582\n",
            "[3/50] Train Loss: 0.1789\n",
            "[4/50] Train Loss: 0.1267\n",
            "[5/50] Train Loss: 0.0933\n",
            "[6/50] Train Loss: 0.0740\n",
            "[7/50] Train Loss: 0.0570\n",
            "[8/50] Train Loss: 0.0457\n",
            "[9/50] Train Loss: 0.0418\n",
            "[10/50] Train Loss: 0.0272\n",
            "[11/50] Train Loss: 0.0366\n",
            "[12/50] Train Loss: 0.0207\n",
            "[13/50] Train Loss: 0.0254\n",
            "[14/50] Train Loss: 0.0216\n",
            "[15/50] Train Loss: 0.0341\n",
            "[16/50] Train Loss: 0.0210\n",
            "[17/50] Train Loss: 0.0103\n",
            "[18/50] Train Loss: 0.0144\n",
            "[19/50] Train Loss: 0.0358\n",
            "[20/50] Train Loss: 0.0067\n",
            "[21/50] Train Loss: 0.0141\n",
            "[22/50] Train Loss: 0.0090\n",
            "[23/50] Train Loss: 0.0140\n",
            "[24/50] Train Loss: 0.0282\n",
            "[25/50] Train Loss: 0.0107\n",
            "[26/50] Train Loss: 0.0344\n",
            "[27/50] Train Loss: 0.0081\n",
            "[28/50] Train Loss: 0.0188\n",
            "[29/50] Train Loss: 0.0051\n",
            "[30/50] Train Loss: 0.0165\n",
            "[31/50] Train Loss: 0.0091\n",
            "[32/50] Train Loss: 0.0020\n",
            "[33/50] Train Loss: 0.0144\n",
            "[34/50] Train Loss: 0.0041\n",
            "[35/50] Train Loss: 0.0291\n",
            "[36/50] Train Loss: 0.0136\n",
            "[37/50] Train Loss: 0.0256\n",
            "[38/50] Train Loss: 0.0051\n",
            "[39/50] Train Loss: 0.0032\n",
            "[40/50] Train Loss: 0.0137\n",
            "[41/50] Train Loss: 0.0143\n",
            "[42/50] Train Loss: 0.0299\n",
            "[43/50] Train Loss: 0.0083\n",
            "[44/50] Train Loss: 0.0093\n",
            "[45/50] Train Loss: 0.0115\n",
            "[46/50] Train Loss: 0.0085\n",
            "[47/50] Train Loss: 0.0028\n",
            "[48/50] Train Loss: 0.0013\n",
            "[49/50] Train Loss: 0.0004\n",
            "[50/50] Train Loss: 0.0002\n",
            "\n",
            "iCNN Test Accuracy: 0.9827\n",
            "\n",
            "iCNN Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      1.00      0.99       620\n",
            "           1       0.99      0.97      0.98       613\n",
            "           2       0.99      0.98      0.98       564\n",
            "\n",
            "    accuracy                           0.98      1797\n",
            "   macro avg       0.98      0.98      0.98      1797\n",
            "weighted avg       0.98      0.98      0.98      1797\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is a comparative analysis of the four models:\n",
        "\n",
        "**bFCNN:**\n",
        "\n",
        "Test Accuracy: 95.72%\n",
        "F1-Score: 0.94 - 0.98\n",
        "Training Loss: Starts at 0.9745 and converges to 0.0595\n",
        "\n",
        "**iFCNN:**\n",
        "\n",
        "Test Accuracy: 95.77%\n",
        "F1-Score: 0.95 - 0.97\n",
        "Training Loss: Starts at 0.7681 and converges to 0.0550\n",
        "\n",
        "**bCNN:**\n",
        "\n",
        "Test Accuracy: 98.05%\n",
        "F1-Score: 0.97 - 0.99\n",
        "Training Loss: Starts at 0.3564 and converges to 0.0000\n",
        "\n",
        "**iCNN:**\n",
        "\n",
        "Test Accuracy: 97.33%\n",
        "F1-Score: 0.96 - 0.98\n",
        "Training Loss: Starts at 0.4418 and converges to 0.0020\n",
        "\n",
        "**Summary:**\n",
        "\n",
        "bCNN has the highest test accuracy (98.05%) and the lowest training loss at convergence (0.0000). It also has a very high F1-Score range of 0.97 - 0.99. iCNN follows closely behind with a test accuracy of 97.33% and a slightly higher training loss at convergence (0.0020). The F1-Score range for iCNN is 0.96 - 0.98.\n",
        "\n",
        "bFCNN and iFCNN have very similar test accuracies (95.72% and 95.77%, respectively) but their F1-Scores are slightly lower than bCNN and iCNN. The training losses at convergence for bFCNN and iFCNN are higher than bCNN and iCNN (0.0595 and 0.0550, respectively).\n",
        "\n",
        "Considering these results, bCNN seems to be the best performing model among the four, followed by iCNN."
      ],
      "metadata": {
        "id": "vy9lUvfEY54e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2"
      ],
      "metadata": {
        "id": "fOtdw3DqWUW8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 1"
      ],
      "metadata": {
        "id": "XveIcRT3Webq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_names = []\n",
        "models = []\n",
        "iCNN_lr00000001 = iCNN()\n",
        "model_names.append(\"iCNN_lr00000001\")\n",
        "models.append(iCNN_lr00000001)\n",
        "\n",
        "iCNN_lr10 = iCNN()\n",
        "model_names.append(\"iCNN_lr10\")\n",
        "models.append(iCNN_lr10)\n",
        "\n",
        "# Train and evaluate each model\n",
        "for model, name in zip(models, model_names):\n",
        "    print(f\"Training {name}...\")\n",
        "    model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=float(name.split(\"lr\")[-1]))\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss = train(model, train_dataloader, criterion, optimizer, device)\n",
        "        print(f\"[{epoch+1}/{num_epochs}] Train Loss: {train_loss:.4f}\")\n",
        "\n",
        "    test_accuracy = evaluate(model, test_dataloader, device)\n",
        "    print(f\"\\n{name} Test Accuracy: {test_accuracy:.4f}\\n\")\n",
        "\n",
        "    # Get predictions and labels\n",
        "    labels, preds = get_predictions(model, test_dataloader, device)\n",
        "\n",
        "    # Print the classification report\n",
        "    print(f\"{name} Classification Report:\")\n",
        "    print(classification_report(labels, preds, zero_division=0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adpQyGpe4gG1",
        "outputId": "069227e4-5fd4-4321-a0c4-1b8f8637e7db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training iCNN_lr00000001...\n",
            "[1/50] Train Loss: 1.1054\n",
            "[2/50] Train Loss: 1.1053\n",
            "[3/50] Train Loss: 1.1052\n",
            "[4/50] Train Loss: 1.1039\n",
            "[5/50] Train Loss: 1.1054\n",
            "[6/50] Train Loss: 1.1044\n",
            "[7/50] Train Loss: 1.1056\n",
            "[8/50] Train Loss: 1.1045\n",
            "[9/50] Train Loss: 1.1043\n",
            "[10/50] Train Loss: 1.1055\n",
            "[11/50] Train Loss: 1.1043\n",
            "[12/50] Train Loss: 1.1041\n",
            "[13/50] Train Loss: 1.1045\n",
            "[14/50] Train Loss: 1.1040\n",
            "[15/50] Train Loss: 1.1039\n",
            "[16/50] Train Loss: 1.1049\n",
            "[17/50] Train Loss: 1.1048\n",
            "[18/50] Train Loss: 1.1060\n",
            "[19/50] Train Loss: 1.1053\n",
            "[20/50] Train Loss: 1.1045\n",
            "[21/50] Train Loss: 1.1044\n",
            "[22/50] Train Loss: 1.1070\n",
            "[23/50] Train Loss: 1.1052\n",
            "[24/50] Train Loss: 1.1053\n",
            "[25/50] Train Loss: 1.1062\n",
            "[26/50] Train Loss: 1.1042\n",
            "[27/50] Train Loss: 1.1052\n",
            "[28/50] Train Loss: 1.1046\n",
            "[29/50] Train Loss: 1.1037\n",
            "[30/50] Train Loss: 1.1045\n",
            "[31/50] Train Loss: 1.1036\n",
            "[32/50] Train Loss: 1.1061\n",
            "[33/50] Train Loss: 1.1042\n",
            "[34/50] Train Loss: 1.1053\n",
            "[35/50] Train Loss: 1.1046\n",
            "[36/50] Train Loss: 1.1054\n",
            "[37/50] Train Loss: 1.1050\n",
            "[38/50] Train Loss: 1.1042\n",
            "[39/50] Train Loss: 1.1039\n",
            "[40/50] Train Loss: 1.1048\n",
            "[41/50] Train Loss: 1.1062\n",
            "[42/50] Train Loss: 1.1045\n",
            "[43/50] Train Loss: 1.1033\n",
            "[44/50] Train Loss: 1.1058\n",
            "[45/50] Train Loss: 1.1060\n",
            "[46/50] Train Loss: 1.1053\n",
            "[47/50] Train Loss: 1.1042\n",
            "[48/50] Train Loss: 1.1051\n",
            "[49/50] Train Loss: 1.1054\n",
            "[50/50] Train Loss: 1.1048\n",
            "\n",
            "iCNN_lr00000001 Test Accuracy: 0.3139\n",
            "\n",
            "iCNN_lr00000001 Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       620\n",
            "           1       0.00      0.00      0.00       613\n",
            "           2       0.31      1.00      0.48       564\n",
            "\n",
            "    accuracy                           0.31      1797\n",
            "   macro avg       0.10      0.33      0.16      1797\n",
            "weighted avg       0.10      0.31      0.15      1797\n",
            "\n",
            "Training iCNN_lr10...\n",
            "[1/50] Train Loss: nan\n",
            "[2/50] Train Loss: nan\n",
            "[3/50] Train Loss: nan\n",
            "[4/50] Train Loss: nan\n",
            "[5/50] Train Loss: nan\n",
            "[6/50] Train Loss: nan\n",
            "[7/50] Train Loss: nan\n",
            "[8/50] Train Loss: nan\n",
            "[9/50] Train Loss: nan\n",
            "[10/50] Train Loss: nan\n",
            "[11/50] Train Loss: nan\n",
            "[12/50] Train Loss: nan\n",
            "[13/50] Train Loss: nan\n",
            "[14/50] Train Loss: nan\n",
            "[15/50] Train Loss: nan\n",
            "[16/50] Train Loss: nan\n",
            "[17/50] Train Loss: nan\n",
            "[18/50] Train Loss: nan\n",
            "[19/50] Train Loss: nan\n",
            "[20/50] Train Loss: nan\n",
            "[21/50] Train Loss: nan\n",
            "[22/50] Train Loss: nan\n",
            "[23/50] Train Loss: nan\n",
            "[24/50] Train Loss: nan\n",
            "[25/50] Train Loss: nan\n",
            "[26/50] Train Loss: nan\n",
            "[27/50] Train Loss: nan\n",
            "[28/50] Train Loss: nan\n",
            "[29/50] Train Loss: nan\n",
            "[30/50] Train Loss: nan\n",
            "[31/50] Train Loss: nan\n",
            "[32/50] Train Loss: nan\n",
            "[33/50] Train Loss: nan\n",
            "[34/50] Train Loss: nan\n",
            "[35/50] Train Loss: nan\n",
            "[36/50] Train Loss: nan\n",
            "[37/50] Train Loss: nan\n",
            "[38/50] Train Loss: nan\n",
            "[39/50] Train Loss: nan\n",
            "[40/50] Train Loss: nan\n",
            "[41/50] Train Loss: nan\n",
            "[42/50] Train Loss: nan\n",
            "[43/50] Train Loss: nan\n",
            "[44/50] Train Loss: nan\n",
            "[45/50] Train Loss: nan\n",
            "[46/50] Train Loss: nan\n",
            "[47/50] Train Loss: nan\n",
            "[48/50] Train Loss: nan\n",
            "[49/50] Train Loss: nan\n",
            "[50/50] Train Loss: nan\n",
            "\n",
            "iCNN_lr10 Test Accuracy: 0.3450\n",
            "\n",
            "iCNN_lr10 Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.35      1.00      0.51       620\n",
            "           1       0.00      0.00      0.00       613\n",
            "           2       0.00      0.00      0.00       564\n",
            "\n",
            "    accuracy                           0.35      1797\n",
            "   macro avg       0.12      0.33      0.17      1797\n",
            "weighted avg       0.12      0.35      0.18      1797\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results of the two training sessions demonstrate the impact of utilizing two considerably different learning rates: 0.00000001 and 10.\n",
        "\n",
        "**For the iCNN_lr00000001 model (with a learning rate of 0.00000001):**\n",
        "\n",
        "The training loss exhibits a gradual decline over 50 epochs, which is anticipated given the minimal learning rate.\n",
        "The test accuracy is 0.3139, indicating suboptimal performance.\n",
        "The classification report reveals that the model is only able to predict class 2 with a moderate degree of success (F1-score of 0.48), while failing to predict class 0 and class 1.\n",
        "\n",
        "**For the iCNN_lr10 model (with a learning rate of 10):**\n",
        "\n",
        "The training loss becomes \"nan\" (not a number) from the first epoch onward, suggesting that the model's weights have likely diverged or exploded as a consequence of the large learning rate.\n",
        "The test accuracy is marginally higher at 0.3450, but remains relatively low.\n",
        "\n",
        "The classification report indicates that the model can only predict class 0 with a moderate degree of success (F1-score of 0.51) and is unable to predict class 1 and class 2.\n",
        "\n",
        "In conclusion, both learning rates result in unsatisfactory performance, albeit for different reasons. The minuscule learning rate of 0.00000001 leads to slow learning, while the substantial learning rate of 10 causes divergence or exploding weights. To enhance the model's performance, it is recommended to experiment with learning rates situated between these two values and conduct a more detailed search for the optimal learning rate."
      ],
      "metadata": {
        "id": "0_yKyyDsarZZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 2"
      ],
      "metadata": {
        "id": "7yfVeQ71Wna8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#### Learning Rate"
      ],
      "metadata": {
        "id": "bTDRBNCjdv68"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A learning rate is a critical hyperparameter in the optimization process of a neural network. It determines the step size taken by the optimizer while updating the model's weights. The choice of the learning rate can significantly influence the model's performance and training time. Below, we discuss the advantages and disadvantages of higher and lower learning rates and their impact on model training and performance.\n",
        "\n",
        "**Higher Learning Rate:**\n",
        "\n",
        "**Advantages:**\n",
        "\n",
        "1. Faster convergence: A higher learning rate allows the model to take larger steps during optimization, which can lead to quicker convergence and shorter training times.\n",
        "\n",
        "2. Escaping local minima: A higher learning rate may enable the model to escape local minima and saddle points in the loss landscape, possibly leading to a better final solution.\n",
        "\n",
        "**Disadvantages:**\n",
        "\n",
        "1. Divergence: An excessively high learning rate may cause the model's weights to diverge or explode, resulting in unstable training and a failure to converge.\n",
        "2. Oscillation: A higher learning rate may cause the optimizer to overshoot the minima, leading to oscillations in the loss landscape and potentially preventing convergence to the optimal solution.\n",
        "3. Suboptimal solutions: The model may converge to a suboptimal solution if the learning rate is too high, as it might not allow the optimizer to fine-tune the weights effectively.\n",
        "\n",
        "**Lower Learning Rate:**\n",
        "\n",
        "**Advantages:**\n",
        "\n",
        "1. Stability: A lower learning rate allows for smaller, more controlled updates to the model's weights, providing greater stability during training.\n",
        "2. Better convergence: A lower learning rate can enable the optimizer to make finer adjustments to the weights, potentially leading to a more accurate solution and better generalization performance.\n",
        "\n",
        "**Disadvantages:**\n",
        "\n",
        "1. Slow convergence: A lower learning rate may result in slow convergence, leading to longer training times and increased computational costs.\n",
        "2. Local minima: A lower learning rate increases the risk of the model becoming stuck in local minima or saddle points, resulting in suboptimal performance.\n",
        "3. Sensitivity to initialization: A lower learning rate may make the model more sensitive to the initial weights, which can affect the quality of the final solution.\n",
        "\n",
        "In conclusion, the choice of an appropriate learning rate is a trade-off between stability and convergence speed. An optimal learning rate should strike a balance between rapid convergence and stability, allowing the model to achieve a high-quality solution in a reasonable amount of time. It is often recommended to experiment with different learning rates, using techniques such as learning rate schedules or adaptive learning rate methods to find the best balance.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4w18m7CAbqgT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Batch Size"
      ],
      "metadata": {
        "id": "TGyh1gXjd0wT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Batch size is another crucial hyperparameter in training neural networks. It refers to the number of samples used to compute the gradient and update the model's weights during each iteration of the optimization process. The choice of batch size can impact the model's performance, convergence speed, and computational requirements. In this discussion, we will explore the advantages and disadvantages of higher and lower batch sizes.\n",
        "\n",
        "**Higher Batch Size:**\n",
        "\n",
        "**Advantages:**\n",
        "\n",
        "1. Computational efficiency: Larger batch sizes lead to better utilization of parallel processing capabilities of modern hardware, such as GPUs, resulting in faster computations per iteration.\n",
        "2. Stable gradient estimates: With a higher batch size, the gradient estimates are more accurate and less noisy, as they are computed using more samples. This can lead to more stable and smooth convergence.\n",
        "\n",
        "**Disadvantages:**\n",
        "\n",
        "1. Memory constraints: Larger batch sizes require more memory to store intermediate values during training, which may limit the model's size or complexity on a given hardware setup.\n",
        "2. Slower convergence: Although each iteration may be faster with a higher batch size, it may require more iterations to converge to an optimal solution, as the model updates less frequently.\n",
        "3. Risk of overfitting: Using a larger batch size may increase the risk of overfitting, as it reduces the inherent regularization effect provided by the noise in the gradient estimates.\n",
        "\n",
        "**Lower Batch Size:**\n",
        "\n",
        "\n",
        "**Advantages:**\n",
        "\n",
        "1. Memory efficiency: Smaller batch sizes require less memory, allowing for the training of larger or more complex models on limited hardware resources.\n",
        "2. Faster convergence: Lower batch sizes result in more frequent weight updates, which can lead to faster convergence in terms of the number of iterations.\n",
        "3. Regularization effect: The inherent noise in the gradient estimates obtained using smaller batch sizes can act as a form of implicit regularization, improving generalization performance.\n",
        "\n",
        "**Disadvantages:**\n",
        "\n",
        "1. Computational inefficiency: Smaller batch sizes may not fully exploit the parallel processing capabilities of modern hardware, leading to less efficient computations per iteration.\n",
        "2. Noisy gradient estimates: Lower batch sizes can result in noisy gradient estimates, as they are computed using fewer samples. This may cause the optimization process to be less stable and more prone to oscillations.\n",
        "\n",
        "In conclusion, the choice of an appropriate batch size involves a trade-off between computational efficiency, memory requirements, and optimization stability. Smaller batch sizes may provide faster convergence and better generalization, but at the cost of computational efficiency and stability. Conversely, larger batch sizes can improve computational efficiency and gradient stability, but may require more memory and increase the risk of overfitting. It is often recommended to experiment with different batch sizes and consider using techniques such as learning rate schedules or adaptive learning rate methods to find the best balance for a given problem and hardware setup."
      ],
      "metadata": {
        "id": "grIMXi73dFWA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exporting PDF"
      ],
      "metadata": {
        "id": "Zwi_QouSPFa9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install pandoc texlive-xetex"
      ],
      "metadata": {
        "id": "eh5D0-s7eQla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp 'drive/My Drive/Comparative Analysis of Neural Networks for COVID Mask Classification.ipynb' './'"
      ],
      "metadata": {
        "id": "VJNiaP2geajB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!jupyter nbconvert --to pdf './Comparative Analysis of Neural Networks for COVID Mask Classification.ipynb' --output-dir=\"./\""
      ],
      "metadata": {
        "id": "giJFa51MedF7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}